{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the device for GPU usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                      else  \"mps\" if torch.backends.mps.is_available()\n",
    "                      else \"cpu\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>great cd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one of the best game music soundtracks   for a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>batteries died within a year ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>works fine, but maha energy is better</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great for the non audiophile</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  polarity\n",
       "0                                           great cd         1\n",
       "1  one of the best game music soundtracks   for a...         1\n",
       "2                   batteries died within a year ...         0\n",
       "3              works fine, but maha energy is better         1\n",
       "4                       great for the non audiophile         1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"../Datasets/Cleaned_Datasets/Dataset_1_test.csv\")\n",
    "new_df=df[[\"title\", \"polarity\"]].copy()\n",
    "new_df = new_df.head(20)\n",
    "new_df.polarity=new_df.polarity-1\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=512\n",
    "TRAIN_BATCH_SIZE=4\n",
    "VALID_BATCH_SIZE=4\n",
    "LEARNING_RATE=1e-05\n",
    "EPOCHS=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a BERT tokenizer from the 'bert-base-uncased' pre-trained model.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonTitles_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class for tokenizing and encoding Amazon product titles\n",
    "    using a given tokenizer and preparing them for model training or inference.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): The input DataFrame containing the titles and polarity labels.\n",
    "        tokenizer: The tokenizer object used to tokenize and encode the titles.\n",
    "        max_len (int): The maximum length of the tokenized titles after encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer=tokenizer\n",
    "        self.data=dataframe\n",
    "        self.titles=dataframe.title\n",
    "        self.targets=self.data.polarity\n",
    "        self.max_len=max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.titles)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        titles=str(self.titles[index])\n",
    "        titles = \" \".join(titles.split())\n",
    "    \n",
    "        inputs=self.tokenizer.encode_plus(\n",
    "            titles,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids=inputs['input_ids']\n",
    "        mask=inputs['attention_mask']\n",
    "        token_type_ids=inputs['token_type_ids'] \n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=0.8\n",
    "train_dataset=new_df.sample(frac=train_size, random_state=200)\n",
    "test_dataset=new_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (20, 2)\n",
      "TRAIN Dataset: (16, 2)\n",
      "TEST Dataset: (4, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = AmazonTitles_Dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = AmazonTitles_Dataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClass(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(BertClass, self).__init__()\n",
    "        self.transformer = transformers.BertModel.from_pretrained('bert-base-cased')\n",
    "        self.drop1 = torch.nn.BatchNorm1d(768)\n",
    "        self.l1 = torch.nn.Linear(768, 300)\n",
    "        self.act1=torch.nn.ReLU()\n",
    "        self.drop2 = torch.nn.Dropout(dropout)\n",
    "        self.l2 = torch.nn.Linear(300, 100)\n",
    "        self.act2=torch.nn.ReLU()\n",
    "        self.drop3=torch.nn.Dropout(dropout)\n",
    "        self.l3=torch.nn.Linear(100,1)\n",
    "        self.output=torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1=self.transformer(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "        output_2=self.drop1(output_1)\n",
    "        output_3=self.drop2(self.act1(self.l1(output_2)))\n",
    "        output_4=self.drop3(self.act2(self.l2(output_3)))\n",
    "        output=self.output(self.l3(output_4))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertClass(\n",
       "  (transformer): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (drop1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (l1): Linear(in_features=768, out_features=300, bias=True)\n",
       "  (act1): ReLU()\n",
       "  (drop2): Dropout(p=0.1, inplace=False)\n",
       "  (l2): Linear(in_features=300, out_features=100, bias=True)\n",
       "  (act2): ReLU()\n",
       "  (drop3): Dropout(p=0.1, inplace=False)\n",
       "  (l3): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (output): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=BertClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCELoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss=0.0\n",
    "    for data in training_loader:\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "        targets = torch.unsqueeze(targets, dim=1)\n",
    "        \n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        #optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate the average loss for the epoch\n",
    "    avg_loss = total_loss / len(training_loader)\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    print(f'Epoch: {epoch}, Average Loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Average Loss: 0.6799671798944473\n",
      "Epoch: 1, Average Loss: 0.6757476180791855\n",
      "Epoch: 2, Average Loss: 0.6686797887086868\n",
      "Epoch: 3, Average Loss: 0.6656881421804428\n",
      "Epoch: 4, Average Loss: 0.6732510328292847\n",
      "Epoch: 5, Average Loss: 0.6653439253568649\n",
      "Epoch: 6, Average Loss: 0.6588905900716782\n",
      "Epoch: 7, Average Loss: 0.6401316374540329\n",
      "Epoch: 8, Average Loss: 0.6562929302453995\n",
      "Epoch: 9, Average Loss: 0.6091750264167786\n",
      "Epoch: 10, Average Loss: 0.6052451729774475\n",
      "Epoch: 11, Average Loss: 0.5858357548713684\n",
      "Epoch: 12, Average Loss: 0.585593581199646\n",
      "Epoch: 13, Average Loss: 0.5744765102863312\n",
      "Epoch: 14, Average Loss: 0.5929204225540161\n",
      "Epoch: 15, Average Loss: 0.5463340431451797\n",
      "Epoch: 16, Average Loss: 0.5244493931531906\n",
      "Epoch: 17, Average Loss: 0.5151379853487015\n",
      "Epoch: 18, Average Loss: 0.5027647018432617\n",
      "Epoch: 19, Average Loss: 0.5277643203735352\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for data in testing_loader:\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            targets = torch.unsqueeze(targets, dim=1)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(outputs.cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 1.0\n",
      "F1 Score (Micro) = 1.0\n",
      "F1 Score (Macro) = 1.0\n"
     ]
    }
   ],
   "source": [
    "outputs, targets = validation(epoch)\n",
    "outputs = np.array(outputs) >= 0.5\n",
    "accuracy = metrics.accuracy_score(targets, outputs)\n",
    "f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "print(f\"Accuracy Score = {accuracy}\")\n",
    "print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "print(f\"F1 Score (Macro) = {f1_score_macro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0], [0.0], [1.0], [1.0]] [[ True]\n",
      " [False]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "print(targets, outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
