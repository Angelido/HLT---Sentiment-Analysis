{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the device for GPU usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                      else  \"mps\" if torch.backends.mps.is_available()\n",
    "                      else \"cpu\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a BERT tokenizer from the 'bert-base-uncased' pre-trained model.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonTitles_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class for tokenizing and encoding Amazon product titles\n",
    "    using a given tokenizer and preparing them for model training or inference.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): The input DataFrame containing the titles and polarity labels.\n",
    "        tokenizer: The tokenizer object used to tokenize and encode the titles.\n",
    "        max_len (int): The maximum length of the tokenized titles after encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer=tokenizer\n",
    "        self.data=dataframe\n",
    "        self.titles=dataframe.title\n",
    "        self.targets=self.data.polarity\n",
    "        self.max.len=max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.titles)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        titles=str(self.titles[index])\n",
    "        titles = \" \".join(titles.split())\n",
    "    \n",
    "        inputs=self.tokenizer.encode_plus(\n",
    "            titles,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids=inputs['input_ids']\n",
    "        mask=inputs['attention_mask']\n",
    "        token_type_ids=inputs['token_type_ids'] \n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.int)\n",
    "        }      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=512\n",
    "BATCH_SIZE=512\n",
    "LEARNING_RATE=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClass(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(BertClass, self).__init__()\n",
    "        self.transformer = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.drop1 = torch.nn.Dropout(dropout)\n",
    "        self.l1 = torch.nn.Linear(768, 300)\n",
    "        self.act1=torch.nn.Tanh()\n",
    "        self.drop2 = torch.nn.Dropout(dropout)\n",
    "        self.l2 = torch.nn.Linear(300, 100)\n",
    "        self.act2=torch.nn.Tanh()\n",
    "        self.drop3=torch.nn.Dropout(dropout)\n",
    "        self.l3=torch.nn.Linear(100,1)\n",
    "        self.out = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        output_1=self.transformer(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "        output_2=self.drop1(output_1)\n",
    "        output_3=self.drop2(self.act1(self.l1(output_2)))\n",
    "        output_4=self.drop3(self.act2(self.l2(output_3)))\n",
    "        output=self.out(self.l3(output_4))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertClass(\n",
       "  (transformer): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (l1): Linear(in_features=768, out_features=300, bias=True)\n",
       "  (act1): Tanh()\n",
       "  (drop2): Dropout(p=0.1, inplace=False)\n",
       "  (l2): Linear(in_features=300, out_features=100, bias=True)\n",
       "  (act2): Tanh()\n",
       "  (drop3): Dropout(p=0.1, inplace=False)\n",
       "  (l3): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (out): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=BertClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCELoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
